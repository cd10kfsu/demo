'use strict';(function(){const b={cache:!0};b.doc={id:'id',field:['title','content'],store:['title','href','section']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/demo/docs/project/lorenz63/',title:"Lorenz63",section:"Project",content:"Data Assimilation Experiments with the Lorenz-63 model #   Download the pdf\nIntroduction #  In Chapter 5, we explored different data assimilation (DA) methods theoretically. This project is intended to give you hands-on experience about their code implementation and get you familiar with different DA methods’ behaviors. We use the famous 3-variable Lorenz-63 model as our toy numerical weather model for its simplicity and its similar chaotic behaviors as the real NWP models.\nLorenz-63 model #  Lorenz (1963) developed a simplified 3-variable model for the atmospheric convection. The equations for the Lorenz-63 model are   \\[\\begin{cases} \\dot{x} = \\sigma (y-x) \u0026amp; \\\\ \\dot{y} = x(\\rho-z) - y \u0026amp; \\\\ \\dot{z} = xy - \\beta z \u0026amp; \\\\ \\end{cases}\\]  where  \\(\\sigma = 10, \\beta = 8/3, \\rho = 28 \\)  . We use the Runge-Kutta 4-stage method to discretize the forward Lorenz-63 model. After getting the forward model, we developed its tangent linear and adjoint model following the instructions in Chapter 5. Figure 1 shows the trajectory generated by the Lorenz-63 model.\nDA system L63-DAS for the Lorenz-63 model #  DA methods included in the L63-DAS #  L63-DAS is a data assimilation package for the Lorenz-63 model. It includes the following DA methods:\n  3D-Var\n  4D-Var with multi-outerloops\n  Perturbed Observation EnKF\n  (Local) Ensemble Transform Kalman Filter (LETKF)\n  All the mentioned methods are already available in the package, and you can directly compare their performance through observing system simulation experiments (OSSE).\nGeneral procedure for OSSE #  Here we briefly introduce the general procedure for OSSE. One advantage of the OSSE is that we have truth (i.e., the true trajectory of the $x, y, z$ for our Lorenz-63 model), which are not available in our real daily applications. Access to the true trajectory (i.e., $x^t, y^t, z^t$) enables us to directly compare the accuracy of the analysis (i.e., $x^a, y^a, z^a$) generated by different DA methods.\nIn L63-DAS, an OSSE includes the following steps:\n  Generate the true trajectory $\\mathbf{x}^t$ from the Lorenz-63 model $\\mathcal{M}$.\n  Create the observations $\\mathbf{y}^o$ by adding Gaussian noise $\\epsilon$ to the simulated quantity from the true trajectory: $\\mathbf{y}^o = \\mathcal{H}(\\mathbf{x}^t)+\\mathbf{\\epsilon}$, where $\\mathbf{\\epsilon} \\sim \\mathcal{N}(0,r^2)$.\n  After setting an initial condition $\\mathbf{x}_0^b$, integrate the Lorenz-63 model from $\\mathbf{x}_0^b$ to get the background $\\mathbf{x}_1^b = \\mathcal{M}(\\mathbf{x}_0^b)$ until the first DA step. Assimilate available observations $\\mathbf{y}_1^o$ at this time step and update the background to the analysis $\\mathbf{x}_1^a$. This analysis will serve the initial condition to get the background for the next DA step: $\\mathbf{x}_2^b = \\mathcal{M}(\\mathbf{x}_1^a)$.\n  Repeat Step 3 for the remaining steps.\n  Finally, you get the whole trajectory of background $\\mathbf{x}^b$ and analysis $\\mathbf{x}^a$. You can compare their accuracy with the truth $\\mathbf{x}^t$ to assess each DA method’s performance.\n  One metric to evaluate the performance of assimilation method is the root mean square error (RMSE). For each assimilation cycle, we have both truth (i.e., $x^t, y^t, z^t$) and analysis (i.e., $x^a, y^a, z^a$). The total RMSE of the analysis at this cycle is defined as\n$$RMSE = \\sqrt{\\frac{1}{3}[(x^t-x^a)^2+(y^t-y^a)^2+(z^t-z^a)^2]}$$\nA short code guide to L63-DAS #  Install the L63-DAS on deepthought2 #    Log into deepthought2 through the command\nssh -X -Y your_user_name@rhel8.deepthought2.umd.edu\n  Load environment by running the following commands\nmodule load intel\nmodule load matlab (if you want to use Matlab)\n  Go to your experiment directory:\ncd /lustre/your_user_name\n  Copy the L63-DAS package to your current directory:\ncp -r /lustre/aosc614-1vb7/L63-DAS .\n  After entering directory L63-DAS, generate executables:\nbash -xe compile_all.bsh\nAfter running this command, you will see fwd.exe, 3dvar.exe, inc4dvar.exe and enda.exe in your current directory.\n  General code structures #  The naming convention is:\n  test_*.f90: main programs for generating executables *.exe.\n  mod_*.f90: modules contain different DA methods and support libraries.\n  compile_*.bsh: compiling scripts for different executables *.exe.\n  The numerical Lorenz-63 model is in mod_lorenz63_fwd.f90 while each data method is built independently as a module in the file mod_lorenz63_[DA_short_name].f90.\nImportant output files #  Each DA subsystem (3dvar.exe, inc4dvar.exe, enda.exe) will output its experiment results in several files:\n  fort.10020: truth of $x, y, z$ for each DA cycle (Each line represents results from one cycle).\n  fort.10030: background of $x, y, z$ for each DA cycle.\n  fort.10040: analysis of $x, y, z$ for each DA cycle.\n  fort.10010: observation of $x, y, z$ for each DA cycle.\n  Since all executables will generate their outputs with the same name, you might want to rename those files immediately after finishing each experiment. You can use the script rename_output.bsh for easy renaming. Its usage is introduced in the next section.\nUseful Utilities #    rename_output.bsh: script for renaming experiment output.\nAfter running each DA methods (those executables ended with .exe), you may want to rename the output for easier post-processing. In this case, you can utilize the script rename_output.bsh. The command is:\nbash rename_output.bsh [filename]\nFor example, you may want to run the command:\nbash rename_output.bsh letkf_m3\nafter you get the results from 3-member LETKF. Now all your experiment results are in the files letkf_m3.100*0.\n  rmse_single_method.m: MATLAB script for a quick check of the time series of the truth, background, analysis trajectory, and analysis errors.\n  Description of this project #  This project has three goals. Using Lorenz-63 model as an example, we will explore:\n  why we need to do DA for NWP applications (Part B)\n  how variational methods work (Part B)\n  how ensemble Kalman filters work (Part C)\n  how hybrid methods work (Part D)\n  Part A: Warm-up #  In this part, we will try to install L63-DAS on $deepthought2$, and then run the Lorenz-63 forward model and generate a trajectory similar to the one in Figure 1.\nTasks #    Follow the instructions in Section 1.3 and install L63-DAS under your directory.\n  Now run the Lorenz-63 forward model to generate a random trajectory.\n(run the command./fwd.exe. It then generates outputs into the file fort.10020)\n  Rename the output as fwd.10020 with the script rename_output.bsh.\n(run the command bash rename_output.bsh fwd)\n  Read the file fwd.10020 and plot the trajectory. Are you able to get something similar to Figure 1?\n  Part B: Variational method #  From this part, we will start to conduct assimilation experiments: We will run OSSEs for different assimilation methods with the L63-DAS. Let’s first explore variational methods.\nRelated codes #  You will need to use 3dvar.exe and inc4dvar.exe in Part B. These two executables receive no interactive inputs, which means you need to modify the source code if you want to change parameters. After you made modification to the source code, remember to re-compile the code to generate new executable for your modifications to take into effect. You can compile the source codes through utility scripts named compile_*.bsh. For these two executables, their main programs, major modules, and stand-alone compiling utilities are:\n  3dvar.exe (lean 3D-Var):\ntest_3dvar.f90 mod_lorenz63_3dvar.f90 compile_3dvar.bsh\n  inc4dvar.exe (incremental 4D-Var with multi-outerloops):\ntest_inc4dvar.f90 mod_lorenz63_inc4dvar.f90 compile_inc4dvar.bsh\n  For all the methods in this part, they use a static background error covariance $\\mathbf{B}$. This matrix is stored in the file fort.1040.\nYou can also use the plotting script rmse_single_method.m to have a quick check of the time series of the true/background/analysis trajectory and analysis errors.\nTasks #    (Necessity of DA) Like the real weather, the Lorenz-63 system is a chaotic system, which means even small discrepancy at the initial time will lead to completely different trajectories at the later time. We will first explore why we need DA. Now let’s see what will happen if we stop doing assimilation (Though we are using 3D-Var as an example, it applies to all DA methods):\n  Perform OSSE with 3D-Var for every cycle and then overlay the truth and analysis trajectory with different colors in a separate panel for each variable $x, y, z$. Finally, plot the total analysis RMSE for each cycle in another panel. From this figure, can the analysis trajectory always be near the truth?\n(hints: Without modifying anything in test_3dvar.f90, run 3dvar.exe and its outputs are in the files fort.100*0. Rename them with rename_output.bsh. Then plot the truth and analysis trajectories. You can use rmse_single_method.m for a quick look.)\n  Now let’s stop doing DA after cycle 3000. Plot the same figure required in (a). Then (1) What do you observe after cycle 3000? (2) How does the RMSE grow after the cycle 3000? (3) Will the RMSE grow to infinity, or its maximum value is bounded?\n(hints: Modify the condition statement above the line “Call lorenz_3dvar(...)” in the source code test_3dvar.f90. After modification, recompile 3dvar.exe with the command bash compile_3dvar.sh. Then run 3dvar.exe)\n  Following (b), if we resume DA after cycle 3500, what will happen?\n  What you observed in this experiment is a universal phenomenon for chaotic systems. It illustrates why we need to do DA continuously.\n  (3D-Var versus 4D-Var) It’s a tremendous advance from 3D-Var to 4D-Var since 4D-Var accounts for the ``error of the day\u0026quot;. In 4D-Var, we assimilate observations from several time slots in each DA cycle, tracing back their impact through the adjoint to the initial background. We will compare the performance of these two methods:\n  Generate 3D-Var analysis and plot the time series of analysis RMSE. Calculate the average RMSE for cycle 3000-4000.\n  Genearte 4D-Var analysis, and then overlay its analysis RMSE over 3D-Var results. Calculate the average RMSE for cycle 3000-4000.\n(Make sure in test_inc4dvar.f90:\nkitermax_out=3,\nnsteps_per_da=2,\nnsteps_da_window=2\nthen recompile inc4dvar.exe with ``bash compile_inc4dvar.sh\u0026quot;, and run inc4dvar.exe)\n  What did you find from this experiment?\n(Tips: The time series of the analysis RMSE will be very noisy. You can further perform a moving average to your raw time series of RMSE. Specifically, for the RMSE $\\sigma_i$ at cycle $\\mathit{i}$, we can generate a smoothed one by\n$$\\sigma_{i}^{smooth} = \\frac{1}{2N+1} \\sum_{-N}^{N} \\sigma_{i+k}$$\nwhere $N$ is the half window size. You can select, for example, $N=20$, and you will see the new RMSE series is smoother than the original series.\n  (4D-Var: impact of the assimilation window length): What if we increase the assimilation window length? How does it influence the analysis? Let’s find it out:\nLet’s fix the outerloop as 3 (set kitermax_out=3 in test_inc4dvar.f90). Then vary the window length from 2 to 4 (i.e., nsteps_per_da=2,nsteps_da_window=2 for window length of 2) and generate 4D-Var analysis for each scenario. Overlay their smoothed analysis RMSE series in one figure, and calculate average analysis RMSE for cycle 3000-4000.\nWhat can you conclude from the results? Remember to recompile inc4dvar.exe for each experiment.\n(Tips:\nYou can wrap all command together to get the result for each experiment. For example, assume you are now running the experiments with the window length of 3. After you set nsteps_per_da=3,nsteps_da_window=3. you can run the command below in one line:\nbash compile_inc4dvar.bsh; ./inc4dvar.exe; bash rename_output.bsh 4dvar_out3_obs3\nThen your 4D-Var results for this case will be saved into files 4dvar_out3_obs3.100*0. )\n  (4D-Var: impact of the outer loop) In all previous 4D-Var experiments, we use three outerloops (kitermax_out=3). But how does the outerloop influence the results? First let’s set a long assimilation window length of 6 (set nsteps_per_da=6,nsteps_da_window=6). Change the number of outerloop (kitermax_out) from 1 to 3 and generate 4D-Var analysis for each case.\n  Overlay their smoothed analysis RMSE series in one figure and calculate average analysis RMSE for cycle 3000-4000.\n  For outerloop equals 1, overlay the truth and analysis trajectory for $x, y, z$.\n  repeat (b) for outerloop is 2 and 3.\n  What did you find? Can you try to explain why this happens?\n(hints: Think about the approximation used in the formulation of incremental 4D-Var, and whether they are still valid for long assimilation window length).\n  Part C: Ensemble method #  We will now focus on the ensemble methods, which also takes the “error of the day” into account while whose developments are much more simple than 4D-Var. In addition to the technical simplicity, ensemble methods also provide us the explicit uncertainty estimates of our analysis as byproducts, which are not available in 4D-Var.\nRelated codes #  For the experiments in Part C, you only need to use enda.exe, which works with you interactively. After typing ./enda.exe in the command line, the program will:\n  ask you to input the observation error, which should be a float number. For all the experiments in this project, input the same value 1.414d0 (where d0 indicates this is a double-precision float number).\n  Then the program will ask you to input the ensemble size. You will need to input an integer from the keyboard (for example, 3).\n  Then the program will ask you to input the inflation parameter, which should be a real number between 0.d0 and 1.d0.\n  Finally, the program will print out a list of all ensemble methods available, and ask you to select one. In the experiments in Part B, you will input 1 for LETKF, 2 for perturbed observation EnKF.\n  After these four inputs, the program will run OSSE, and the results are written into files, as indicated in Section 1.3.3.\nTasks #  For all the following experiments, the observation errors are all set with a value of 1.414d0.\n  (Compare 3D-Var, 4D-Var and LETKF)\n  Use 10-member LETKF (filter 1 in enda.exe) with no inflation (value 0.d0 for inflation parameter). Then in the figure overlay its analysis RMSE with 3D-Var and 4D-Var. Compare their analysis RMSE for cycle 3000-4000\nHow is its performance compared with the variational methods?\nNote that since we have ensemble members now, we can explicitly calculate the covariance between any pair of model states (e.g., $x$ and $y$) for any time step. By contrast, though the background error covariance in 4D-Var will evolve with time, we cannot write out the varying covariance that contains the ``error of the day\u0026quot; at later time steps.\n  Now use 3-member LETKF (filter 1 in enda.exe) with no inflation (value 0.d0 for inflation parameter). Then plot the same figure in (a). How is its performance compared with the variational methods now? If comparing its analysis trajectory with truth, what did you find?\n    (Remedy to small ensemble size) In 1(b), we see the ensemble Kalman filter encounters problems when the ensemble size is small compared to the dimension of the model state (i.e., 3). In real applications, the dimension of model state (i.e., model state of the GFS) is far greater than the ensemble size we use ($N_{ensemble}\\sim$ hundreds). How do we fix this problem?\nThe ensemble DA community developed several simple but effective tools to deal with this problem. Among them, one is called the Relaxation to Prior Perturbation (RTPP, Zhang et al., 2004). The scheme itself is extremely simple, after we get the m-member analysis perturbations $\\Delta\\mathbf{X}^a=[\\delta\\mathbf{x}^{a,1}|\\delta\\mathbf{x}^{a,2}|\u0026hellip;|\\delta\\mathbf{x}^{a,m}]$. We create the final analysis perturbation by merging this analysis perturbation and background perturbation, which are $$\\Delta\\mathbf{X}_{final}^a = \\alpha \\Delta\\mathbf{X}^b + (1-\\alpha) \\Delta\\mathbf{X}^a$$ Then we get the final analysis members by adding these final analysis perturbations back to the ensemble analysis mean.\nNow let’s see if this simple method works. We still use 3-member LETKF, but this time with an inflation parameter of 0.5d0. Check the truth and analysis trajectory, does the LETKF work correctly now?\n  (Partial observation coverage) In DA, the background error covariance plays an important role in sharing information between different variables. Even some variables are not directly observed, through the background error covariance, they can still be updated through observations for other variables. We will explore how LETKF works with partial observation coverage:\n  Start with 5-member LETKF with no inflation, remove observation for variable $z$, and run the experiment. Plot the truth and analysis trajectory for each variable. (In test_enda.f90, change the last .true. to .false. in the call ``Call lorenz63_letkf(...)\u0026quot;. After modification, recompile enda.exe and then ./enda.exe)\nIs the LETKF able to work correctly? How is its average analysis RMSE compared with full observation coverage for cycle 3000-4000?\n  Still start with 5-member LETKF with no inflation, but this time remove observation for variable $y$ and rerun the LETKF experiments.\nCan the LETKF work correctly now? If not, can you think of a way to make it work without adding new observations?\n    (stochastic EnKF vs. deterministic EnKF) There are two types of ensemble Kalman filters. The perturbed observation EnKF belongs to the stochastic EnKF, while LETKF is formulated in a deterministic manner. Their different formulations lead to their different performances.\nIn this experiment, we will use no inflation. For both deterministic filter (1 for LETKF, ) and stochastic filter (2 for perturbed observation EnKF):\n  calculate their average RMSE for cycle 3000-4000 for three ensemble sizes (3, 10, 15).\n  plot the analysis trajectory over the truth trajectory for these two filters for the ensemble size 10.\n  From the results of 4(a) and 4(b), what did you find?\n  Part D: Hybrid method #  Penny [2014] developed a hyrbid method (hybrid-gain) that combines both the variational method and ensemble method. Unlike hybrid-covariance method, hybrid-gain method requires minimal code modifications if users already have a variational and ensemble system. In the hybrid-gain method that combines 3D-Var and LETKF, we try to seek the final analysis ${\\mathbf{x}}_{HG}^a$ as  \\[ \\begin{split} min\\; J(\\mathbf{x}_{3DVar}^a) = \u0026amp; (\\mathbf{x}_{3DVar}^a-\\Bar{\\mathbf{x}}_{LETKF}^a)^T \\mathbf{B}_{3DVar}^{-1}(\\mathbf{x}_{3DVar}^a-\\Bar{\\mathbf{x}}_{LETKF}^a) \u0026#43; \\\\ \u0026amp; (\\mathbf{y}^o-\\mathcal{H}(\\mathbf{x}_{3DVar}^a))^T \\mathbf{R}^{-1}(\\mathbf{y}^o-\\mathcal{H}(\\mathbf{x}_{3DVar}^a)) \\end{split}\\]  $${\\mathbf{x}}{HG}^a = \\alpha \\Bar{\\mathbf{x}}{LETKF}^a + (1-\\alpha)\\mathbf{x}_{3DVar}^a$$\nThe equations above can be summarized as the following procedure for each DA cycle:\n  Assume we already got the m-member ensemble background $ \\mathbf{X}^b=[\\mathbf{x}^{b,1}|\\mathbf{x}^{b,2}|\u0026hellip;|\\mathbf{x}^{b,m}]$, where each column of $\\mathbf{X}^b$ represent one member.\n  Use LETKF to get the LETKF analysis mean $\\Bar{\\mathbf{x}}{LETKF}^a$ and analysis perturbations $$\\Delta\\mathbf{X}{LETKF}^a=[\\delta\\mathbf{x}{LETKF}^{a,1}|\\delta\\mathbf{x}{LETKF}^{a,2}|\u0026hellip;|\\delta\\mathbf{x}{LETKF}^{a,m}]$$ where $\\delta\\mathbf{x}{LETKF}^{a,k}=\\mathbf{x}{LETKF}^{a,k}-\\Bar{\\mathbf{x}}{LETKF}^a$, k=1,2,\u0026hellip;,m.\n  Set $\\Bar{\\mathbf{x}}{LETKF}^a$ as the background for the 3D-Var: $\\mathbf{x}{3DVar}^b=\\Bar{\\mathbf{x}}{LETKF}^a$, and 3D-Var will then generate its analysis $\\mathbf{x}{3DVar}^a$.\n  Generate the hybrid-gain analysis ${\\mathbf{x}}{HG}^a = \\alpha \\Bar{\\mathbf{x}}{LETKF}^a + (1-\\alpha)\\mathbf{x}_{3DVar}^a$.\n  Generate the final m-member ensemble HG analysis members $ \\mathbf{X}{HG}^a=[\\mathbf{x}{HG}^{a,1}|\\mathbf{x}{HG}^{a,2}|\u0026hellip;|\\mathbf{x}{HG}^{a,m}]$, where $\\mathbf{x}{HG}^{a,k}=\\mathbf{x}{HG}^a+\\delta\\mathbf{x}_{LETKF}^{a,k}, k=1,2,\u0026hellip;,m$\n  Evolve each HG analysis member through NWP model to get the background at the next DA step.\n  Tasks #  In this part, we will build a hybrid-gain system based on the existing 3D-Var system (test_3dvar.f90) and LETKF (test_enda.f90) and performing two experiments:\n  Build a hybrid-gain system that combines 3D-Var and LETKF.\n  Now let’s play with the hybrid-gain method:\n  (Hybrid-gain vs. EnKF) With full observation coverage, use only 2 or 3 members and no inflation, respectively run LETKF and your hybrid-gain system. For each method, overlay their analysis trajectory over the truth trajectory. For the hybrid-gain method, use $\\alpha=0.5$.\n  (Hybrid-gain vs. 3D-Var) following (2), now compare your smoothed Hybrid-gain analysis RMSE with 3D-Var RMSE in the same figure. Compare their average RMSE for cycle 3000-4000.\n  What did you find from 2(a) and 2(b)?\n  Coding tips #    To implement hybrid-gain, you can make a copy of test_enda.f90 and name it as test_hg.f90 and then try to insert 3D-Var subroutine lorenz63_3dvar into test_hg.f90.\n  3D-Var subroutine lorenz63_3dvar is in the code mod_lorenz63_3dvar.f90. LETKF subroutine lorenz63_letkf is in the code mod_lorenz63_letkf.f90.\n  After finishing writing the main program test_hg.f90, you will need to compile your code with the script compile_hg.bsh. If your code has no coding error, you will see an executable named hg.exe under the directory. And you will use this executable to run the hybrid-gain method. Remember to select filter 1 for the LETKF code section when running hg.exe.\n  (Optional) Part E: Information transport in a DA system #  In this part, we try to figure out how observations adjust background model state through different components (i.e., $\\mathbf{B}, \\mathbf{H}, \\mathbf{L}$) in a DA system. Knowing how each component modifies the background in a DA system enables us to trace back the components that failed to work realistically when we get an inferior analysis. For example, in carbon data assimilation, the variable localization method [Kang et al., 2011] significantly improve carbon flux estimation by removing unrealistic ensemble inter-variable covariance due to small ensemble size. As the first step, we will try to understand how those DA components transport information in different scenarios.\nTasks #  As shown in the Introduction, Lorenz-63 system includes three model variables (i.e., x, y, z), so their background error covariance matrix $\\mathbf{B}$ is a $3 \\times 3$ matrix. For the following scenarios, please tell if the background will be updated due to the assimilation of observations. Specifically, you need to determine: (1) if $x^a \\neq x^b$, (2) if $y^a \\neq y^b$, and (3) if $z^a \\neq z^b$. Please also briefly explain your answer. For all the scenarios, please assume that the background value never equals observation value(i.e., $x^b \\neq x^o, y^b \\neq y^o, z^b \\neq z^o$), and the observation errors (i.e., $\\mathbf{R}$) are nonzero.\n(hints: Each scenario includes at least one special DA component so that you can make quick judgements without calculation. If you cannot intuitively answer them, you can refer to KF/OI/3D-Var equations.)\n  (Scenario 1) Assuming you are using a OI/3D-Var system for the Lorenz-63 equations for one DA cycle. For this cycle, you have direct observations $x^o, y^o, z^o$, and your $\\mathbf{B}$ has the structure $\\begin{bmatrix} e_{xx} \u0026amp; e_{xy} \u0026amp; e_{xz}\\\ne_{xy} \u0026amp; e_{yy} \u0026amp; e_{yz} \\\ne_{xz} \u0026amp; e_{yz} \u0026amp; e_{zz} \\end{bmatrix}$ where elements started with $e$ are nonzero.\n  (Scenario 2) Assuming you are using a OI/3D-Var system for the Lorenz-63 equations for one DA cycle. For this cycle, you only have direct observations $x^o$, and your $\\mathbf{B}$ has the structure $\\begin{bmatrix} e_{xx} \u0026amp; e_{xy} \u0026amp; e_{xz}\\\ne_{xy} \u0026amp; e_{yy} \u0026amp; e_{yz} \\\ne_{xz} \u0026amp; e_{yz} \u0026amp; e_{zz} \\end{bmatrix}$ where elements started with $e$ are nonzero.\n  (Scenario 3) Assuming you are using a OI/3D-Var system for the Lorenz-63 equations for one DA cycle. For this cycle, you only have direct observations $x^o$, and your $\\mathbf{B}$ has the structure $\\begin{bmatrix} e_{xx} \u0026amp; e_{xy} \u0026amp; 0 \\\ne_{xy} \u0026amp; e_{yy} \u0026amp; e_{yz} \\\n0 \u0026amp; e_{yz} \u0026amp; e_{zz} \\end{bmatrix}$ where elements started with $e$ are nonzero.\n  (Scenario 4) Assuming you are using a OI/3D-Var system for the Lorenz-63 equations for one DA cycle. For this cycle, you only have direct observations $x^o$, and your $\\mathbf{B}$ has the structure $\\begin{bmatrix} e_{xx} \u0026amp; 0 \u0026amp; 0 \\\n0 \u0026amp; e_{yy} \u0026amp; 0\\\n0 \u0026amp; 0 \u0026amp; e_{zz} \\end{bmatrix}$ where elements started with $e$ are nonzero.\n  (Scenario 5) Assuming you are using a OI/3D-Var system for the Lorenz-63 equations for one DA cycle. For this cycle, your $\\mathbf{B}$ has the structure $\\begin{bmatrix} e_{xx} \u0026amp; 0 \u0026amp; 0 \\\n0 \u0026amp; e_{yy} \u0026amp; 0\\\n0 \u0026amp; 0 \u0026amp; e_{zz} \\end{bmatrix}$ where elements started with $e$ are nonzero. However, in this case, you don’t have any direct observations. Instead, you have one observation $q^o=h(x,y)=x^t+y^t+\\epsilon$, where $x^t, y^t$ are true value of model state $x, y$, and $\\epsilon$ a Gaussian noise.\n  (Scenario 6) Assuming you are now using a 4D-Var system for the Lorenz-63 equations for one DA cycle. For this cycle, your $\\mathbf{B_0}$ has the structure  \\( \\begin{bmatrix} e_{xx} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; e_{yy} \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; e_{zz} \\end{bmatrix} \\)  where elements started with $e$ are nonzero. Still, you only have direct observations for model state $x$. But since you are using 4D-Var, the assimilation window length is set as 6, which means now you actually have $x$ observations from 6 time slots (i.e.,$x_{1}^o,x_{2}^o,x_{3}^o,x_{4}^o,x_{5}^o,x_{6}^o$).\n  "}),a.add({id:1,href:'/demo/docs/',title:"Project",section:"",content:"project #  "}),a.add({id:2,href:'/demo/docs/project/',title:"Project",section:"Project",content:"project #  "}),a.add({id:3,href:'/demo/docs/supplemental/',title:"supplemental",section:"Project",content:"sup #  "}),a.add({id:4,href:'/demo/docs/welcome/',title:"Welcome!",section:"Project",content:" Welcome! #  This is the website for the book \u0026ldquo;Earth system modeling, data assimilation, and predictability\u0026rdquo;\n"}),a.add({id:5,href:'/demo/docs/appendix/appendixA/',title:"Appendix A",section:"Appendix",content:"Appendix A: The early history of NWP #  Notes written from memory by AndersPersson (ECMWF) on 16 September 1999. The reader is encouraged to read P. D. Thompson’s paper “Charney and the Revival of Numerical Weather Prediction”, reproduced, together with Charney’s letters to Thompson in Lindzen et al., (1990).\nHistory of NWP #  In late 1945 Vladimir Zworykin, the “Father of Television”, who worked at RCA, joined with John von Neumann, the “Father of the Computer”, to suggest the use of the computer in meteorology. Zworykin’s interest was in weather modification, and von Neumann’s was in fluid dynamics. They also had the dream of connecting the TV and the computer into something we today know as a PC or Workstation. Their dream came partially true in Sweden in around 1955 when for the first time a forecast map that was made directly and automatically without any human intervention was produced on as creen (oscilloscope) (see Bergthorssonand Doös,(1955), Bergthorsson et al., (1955), also the Rossby Memorial Volume).\nIn early 1946 von Neumann contacted Rossby’s group. They told von Neumann why a zonally averaged dynamical model would not work, and instead suggested a barotropic model which had been manually tested by Victor Starr in his 1941 book on weather forecasting for a 72-h forecast at 700 hPa. Von Neumann was not satisfied with the simple barotropic approach and in speeches in the spring of 1946 presented more ambitious plans. Von Neumann and Zworykin also appeared at the annual meeting of the AMS (see Bulletin of AMS (1946)).\nIn the summer of 1946 the Princeton meeting took place. Few if any had any idea of what should be done. Not even the normally optimistic Rossby could see a solution to the problem. A working group was set up with Albert Cahn and Phil Thompson, with Hans Panofsky and Bernhard Haurwitz acting as advisors. By the autumn of 1946, there was still no clear idea of what to do. Cahn left meteorology to become a successful real estate agent in California, leaving Phil Thompson in despair. It was at this crucial state that Jule Charney moved to Chicago (on his way to Norway). Charney had attended the Princeton meeting, where he had offered some obscure ideas about having the whole atmosphere represented by a few singular levels.\nIn early 1947, Charney, now in Oslo, wrote to Phil Thompson that he indeed saw light at the end of the tunnel taking a completely new approach. It is important to realize that the practical(political/psychological)impact of L.F.Richardson’s 1922 book was essentially to convince the meteorological community that NWP was impossible. This was further supported by the experience Phil Thompson and others had while trying to make use of Jack Bjerknes “tendency equation” (which was as much in vogue then as potential vorticity is today!). For the 1948–50 events, I refer to the well-known literature.\nWhy Sweden? #  The first real-time, operational NWP was run in Sweden in September 1954 (to 72 h at 500 hPa), half a year before the USA.\nTwo Reasons:\n For a short period in 1954 the Swedes were in possession of the world’s most powerful computer, BESK. In 1950 they had already constructed a more basic one, BARK. One must again realize the thinking at that time: even among the most radical, it was felt that having just one computer in Sweden for the coming 20–30 years was sufficient. Even in the USA they thought that four or five computers would be more than enough for the foreseeable future. The “explosion” only came in 1955 when IBM launched their first machine. Rossby moved to Sweden and wanted to repeat the ENIAC success of 1950 in his homeland.In this endeavor he was supported by:(a)the Swedish Air force and other national institutions (but not the Meteorological Service!); (b)young enthusiastic scientists who worked at or visited his institutions, both Swedish and foreign;(c) the US Air Force and Woods Hole.  (See articles by Wiin-Nielsen in Tellus, 1991, and Bolin in Tellus, 1999).\nThe Swedish project was hampered or complicated by an internal political conflict. In1954 a new Director of SMHI(the Swedish Meteorological Office) was to be elected by the government. Rossby would have been the obvious choice, but he was seen as a troublemaker. The “official” candidate was Alf Nyberg, who had taken a very skeptical attitude towards Rossby’s project. Against him, Rossby lobbied Herrlin, head of the Military Meteorological Service. Unfortunately the run-up to the selection of new Director coincided with the launch of the first real-time operational NWP, 29 Sep–2 Oct 1954. Those who supported Nyberg took a negative attitude; those who supported Rossby took a positive one. In the end, the government chose Nyberg. SMHI began slowly to support NWP 5 day/week barotropic forecasts to 72 h at 500 hPa started in early December 1954. The US operational NWP started in May 1955, but it was not until 1958 that they reached the same quality standard as the Swedish. Japan started in 1959 along the same lines as Sweden.\nMore provocative ideas #  Between 1950 (the first ENIAC run) and 1955 (the start of operational NWP) there was a long lapse of 5 years. Why? To what extent was the delay due to computer resources? To what degree to skepticism about NWP? Charney’s presentation in his 1954 National Academy of Sciences paper is very political. My feeling from this paper and other sources is that he and the meteorological community were under strong pressure to present results, in particular with respect to the Thanksgiving Storm of 1950. The set up of the committee for NWP with George Cressman as Head seems to have been done in great haste. The Swedes were known to be progressing towards operational NWP. During the “Dark Years” 1956–57 some influential persons relying on Norbert Wiener suggested that computers should be used for statistical forecasting of weather patterns. Ed Lorenz at MIT was given the task of finding out if nonlinear dynamic evolutions could be reproduced or simulated by statistical means. His report, of which I have a copy, was guardedly optimistic! If Phillips and Cressman in 1957–58 had not managed to develop a functioning NWP system, things might really have developed along other lines. . . . It was during this or related work that Lorenz discovered the Butterfly Effect. As mentioned by Phillips in his 1990 monograph about energy dispersion (Phillips 1990a), if Charney et al. had run the ENIAC forecasts on a small area, the whole experiment would have had a severe setback, similar to Richardson’s 1922 work. It is not commonly known that the UKMO lost 15years (1950–65) by trying to run a (good) baroclinic model on an area that was too small.\n"}),a.add({id:6,href:'/demo/docs/homework/homework5/',title:"Homework5",section:"Homework",content:"homework 5 #  "}),a.add({id:7,href:'/demo/docs/project/lorenz96/',title:"Lorenz96",section:"Project",content:"Lorenz-96\n"}),a.add({id:8,href:'/demo/docs/project/SPEEDY/',title:"Speedy",section:"Project",content:"SPEEDY #  "}),a.add({id:9,href:'/demo/docs/supplemental/nmi/',title:"Nmi",section:"supplemental",content:"Normal Mode Initialization #  "})})()